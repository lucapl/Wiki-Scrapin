{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki scraper report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author:\n",
    "≈Åukasz Andryszewski 151930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "The project consited of:\n",
    "- Creating a database of 1000 wikipedia articles\n",
    "- Scraping/Lemmatizing them\n",
    "- Creating a recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the database\n",
    "\n",
    "The database should consist of a wide variety of topics, so the articles should be randomized. This could be achieved by generating 1000 random words and then searching using: \n",
    "- https://en.wikipedia.org/w/index.php?search=INPUT_TEXT\n",
    "\n",
    "Thankfully wikipedia and fandom offer special pages among which, a random article can be entered:\n",
    "- https://en.wikipedia.org/wiki/Special:Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the data the following parser was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "def get_title_and_text(url='https://en.wikipedia.org/wiki/Special:Random'):\n",
    "    response = requests.get(url)\n",
    "    parsed = bs4.BeautifulSoup(response.text,features=\"lxml\")\n",
    "    output = \"\"\n",
    "    for p in parsed.select('p'):\n",
    "        output += p.getText()\n",
    "    return parsed.title.string,response.url,output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GET, it loads the html site and joins all paragraphs into one long string. However to ensure the data is interesting if an article containted less than 2000 characters it was exchanged for another article. This is done to exclude bland articles like:\n",
    "- https://en.wikipedia.org/wiki/1296_in_poetry\n",
    "\n",
    "The created database is stored as a simple csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizng\n",
    "\n",
    "To reduce the size of the data and get rid of useless words a word lemmatizer is applied and stopwords are filtered out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemmatizer(string,method):\n",
    "    stops = stopwords.words('english')\n",
    "    return list(filter(lambda s: not s in stops,map(WordNetLemmatizer().lemmatize,word_tokenize(string))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Lemmatizer is used instead of a Stemmer because of issues in articles interpretability.\n",
    "\n",
    "Using this methods 1000 articles were downloaded and put into a csv files along with their urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System\n",
    "\n",
    "To recommend articles a similarity measure must be established between them. The articles are first scored using TF-IDF. The score is calculated the following way:\n",
    "\n",
    "$$ tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) $$\n",
    "\n",
    "$ tf(t,d) $ - term frequency in a given document\n",
    "\n",
    "$ idf(t,D) $ - information the term provides in a domain of documents\n",
    "\n",
    "$t$ - term, $d$ - document, $D$ - domain of documents\n",
    "\n",
    "The used vectorizer comes from sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf=TfidfVectorizer(use_idf=True, smooth_idf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between documents is calculated using the cosine distance measure on the normalized tfidf values. The distance measured is then substracted from 1 transforming it into a similarity measure.\n",
    "\n",
    "The recommender:\n",
    "1. downloads the text from the article\n",
    "2. lemmatizes the article\n",
    "2. transform the query articles using tfidf \n",
    "3. measures similarity to each article in the database\n",
    "3. returns n most similar articles from the database, along with their similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(df,tfidf,queries,n=3):\n",
    "    recommended = {}\n",
    "    q_arrays = []\n",
    "\n",
    "    for query in queries:\n",
    "        _,url,text = get_title_and_text(query)\n",
    "        q_stem = \" \".join(word_lemmatizer(text))\n",
    "        q_array = tfidf.transform([q_stem]).toarray()[0]\n",
    "        q_arrays.append(q_array)\n",
    "        values = (1-df.apply(lambda x: cosine(x, q_array), axis=1).sort_values())\n",
    "        recommended[query] = (values[:n])\n",
    "\n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
